{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGKymuh+oMOy+ZJeZjwJcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takeisika/group-89-eeg-depression/blob/main/eeg_pred_model_dev_share.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "464SROaR_HmJ",
        "outputId": "9b82b01a-b6d6-4584-d28f-509d34c9c623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/EEG_128channels_resting_lanzhou_2015.zip -d /content/data"
      ],
      "metadata": {
        "id": "0-LrdJbU_Ps4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d0dbc4-5ec7-4348-a332-6c604d6e8b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/EEG_128channels_resting_lanzhou_2015.zip\n",
            "   creating: /content/data/EEG_128channels_resting_lanzhou_2015/\n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010013rest 20150703 1333..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010012rest 20150626 1026..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020022rest 20150707 1452..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/Multivariate Pattern Analysis of EEG-Based Functional Connectivity A Study on the Identification of Depression.pdf  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010019rest 20150716 1440..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02030020_rest 20151230 1416.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010005rest 20150507 0907..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010022restnew 20150724 14.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010033rest 20160331 1239..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010015rest 20150709 1456..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020029rest 20150715 1316..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020026_rest 20150714 1413.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010026rest 20160311 1421..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010006rest 20150528 0928..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02030014rest 20151117 1441..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02030021rest 20160105 1141..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02030004_rest 20151026 1930.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010036_rest 20160408 1418.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020023restnew 20150709 10.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02010025 20160311 1206.mat.mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020018rest 20150702 1651..mat  \n",
            "  inflating: /content/data/EEG_128channels_resting_lanzhou_2015/02020027rest 20150713 1049..mat  "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ① Get all .mat files"
      ],
      "metadata": {
        "id": "vZaXf9OLJeiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "FOLDER = \"/content/data\"\n",
        "mat_files = glob.glob(os.path.join(FOLDER, \"**/*.mat\"), recursive=True)\n",
        "print(f\"Found {len(mat_files)} .mat files.\")"
      ],
      "metadata": {
        "id": "biM6MrYiJgwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04269882-8060-4655-adc7-325610b11ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 53 .mat files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ② Get subj_id & its label"
      ],
      "metadata": {
        "id": "J7C4r2DVVk_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "def get_subj_id_and_label(path):\n",
        "    filename = os.path.basename(path)\n",
        "    match = re.search(r\"(02\\d+)\", filename)  # 02010002_... or 02020008_... or 02030002_...\n",
        "    subj_id = match.group(1)\n",
        "    label = 1 if subj_id.startswith(\"0201\") else 0  # 0201...→depressed (1), others→not depressed(0)\n",
        "    return subj_id, label"
      ],
      "metadata": {
        "id": "5ZF1-5V4JjuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ③ Preprocessing"
      ],
      "metadata": {
        "id": "u5cf5lO0K0g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import signal\n",
        "import random\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "SAMPLING_RATE = 250\n",
        "LOW_CUTOFF = 1.0\n",
        "HIGH_CUTOFF = 45.0\n",
        "NOTCH = 50.0\n",
        "TRIM_SEC = 30\n",
        "\n",
        "WIN_SEC = 2.0\n",
        "STEP_SEC = WIN_SEC / 2\n",
        "\n",
        "def butter_bandpass_filter(data, sampling_rate, low_cutoff, high_cutoff, filter_order=4):\n",
        "    nyquist_freq = 0.5 * sampling_rate\n",
        "    low_norm = low_cutoff / nyquist_freq\n",
        "    high_norm = high_cutoff / nyquist_freq\n",
        "    b_coeffs, a_coeffs = signal.butter(filter_order, [low_norm, high_norm], btype='band')\n",
        "    filtered_data = signal.filtfilt(b_coeffs, a_coeffs, data, axis=-1)\n",
        "    return filtered_data\n",
        "\n",
        "def notch_filter(data, sampling_rate, notch, quality_factor=30.0):\n",
        "    nyquist_freq = 0.5 * sampling_rate\n",
        "    notch_norm = notch / nyquist_freq\n",
        "    b_coeffs, a_coeffs = signal.iirnotch(w0=notch_norm, Q=quality_factor)\n",
        "    filtered_data = signal.filtfilt(b_coeffs, a_coeffs, data, axis=-1)\n",
        "    return filtered_data\n",
        "\n",
        "def preprocess_data(data, sampling_rate, low_cutoff=LOW_CUTOFF, high_cutoff=HIGH_CUTOFF, notch=NOTCH, trim_sec=TRIM_SEC):\n",
        "    processed_data = data - data.mean(axis=0, keepdims=True)\n",
        "    processed_data = butter_bandpass_filter(processed_data, sampling_rate, low_cutoff, high_cutoff)\n",
        "    processed_data = notch_filter(processed_data, sampling_rate, notch)\n",
        "    start_sample = int(trim_sec * sampling_rate)\n",
        "    end_sample = processed_data.shape[1] - int(trim_sec * sampling_rate)\n",
        "    end_sample = max(end_sample, start_sample + 1)\n",
        "    return processed_data[:, start_sample:end_sample]\n",
        "\n",
        "def slide_wins(preprocessed_data, sampling_rate, win_sec=WIN_SEC, step_sec=STEP_SEC):\n",
        "    win_cnts = int(win_sec * sampling_rate)\n",
        "    step_cnts = int(step_sec * sampling_rate)\n",
        "\n",
        "    wins = []\n",
        "    win_idxs = []\n",
        "    for start_idx in range(0, preprocessed_data.shape[1] - win_cnts + 1, step_cnts):\n",
        "        end_idx = start_idx + win_cnts\n",
        "        wins.append(preprocessed_data[:, start_idx:end_idx])\n",
        "        win_idxs.append((start_idx, end_idx))\n",
        "\n",
        "    if wins:\n",
        "        wins_array = np.stack(wins, axis=0)\n",
        "    else:\n",
        "        wins_array = np.empty((0, preprocessed_data.shape[0], win_cnts))\n",
        "\n",
        "    return wins_array, win_idxs\n",
        "\n",
        "def discard_noisy_wins(wins_array, z_score_threshold=7.0):\n",
        "    if len(wins_array) == 0:\n",
        "        return wins_array\n",
        "    mean = wins_array.mean(axis=(1, 2), keepdims=True)\n",
        "    std = wins_array.std(axis=(1, 2), keepdims=True) + 1e-6\n",
        "    z_scores = (wins_array - mean) / std\n",
        "    is_not_noisy_win_tf = np.max(np.abs(z_scores), axis=(1, 2)) < z_score_threshold\n",
        "    return wins_array[is_not_noisy_win_tf]"
      ],
      "metadata": {
        "id": "aS9P_V3jK2-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ④ Feature Extraction (Welch's Method)"
      ],
      "metadata": {
        "id": "xa98ZdUNK71d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FREQ_BANDS = [\n",
        "    (1, 4),    # Delta (δ)\n",
        "    (4, 8),    # Theta (θ)\n",
        "    (8, 13),   # Alpha (α)\n",
        "    (13, 30),  # Beta (β)\n",
        "    (30, 45)   # Gamma (γ)\n",
        "]\n",
        "\n",
        "def welch(wins_array, sampling_rate=SAMPLING_RATE, freq_bands=FREQ_BANDS):\n",
        "    if len(wins_array) == 0:\n",
        "        return np.empty((0, 0), dtype=np.float32)\n",
        "\n",
        "    win_cnts, ch_cnts, sample_cnts = wins_array.shape\n",
        "    samples_per_seg = 256\n",
        "    features = []\n",
        "\n",
        "    for idx in range(win_cnts):\n",
        "        curr_win = wins_array[idx]\n",
        "        freqs, psds = signal.welch(curr_win, fs=sampling_rate, nperseg=samples_per_seg, axis=-1)\n",
        "        integrated_psd_in_all_freqs = np.trapezoid(psds, freqs, axis=-1) + 1e-12\n",
        "\n",
        "        curr_win_bands = []\n",
        "        for (low_freq, high_freq) in freq_bands:\n",
        "            is_in_this_freq_band_tf = (freqs >= low_freq) & (freqs < high_freq)\n",
        "            integrated_psd_in_this_freq_band = np.trapezoid(psds[:, is_in_this_freq_band_tf], freqs[is_in_this_freq_band_tf], axis=-1)\n",
        "            relative_psd_in_this_freq_band_in_perc = integrated_psd_in_this_freq_band / integrated_psd_in_all_freqs\n",
        "            curr_win_bands.append(relative_psd_in_this_freq_band_in_perc)\n",
        "\n",
        "        win_features = np.stack(curr_win_bands, axis=-1)\n",
        "        features.append(win_features)\n",
        "\n",
        "    features_array = np.stack(features, axis=0)\n",
        "    features_array = features_array.reshape(win_cnts, -1).astype(np.float32)\n",
        "\n",
        "    return features_array"
      ],
      "metadata": {
        "id": "u5NcBY7JK9xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⑤ Load Target Data from .mat Files"
      ],
      "metadata": {
        "id": "g2S9MV48OeCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "\n",
        "def load_mat(path, verbose=True):\n",
        "    mat = loadmat(path, squeeze_me=True, struct_as_record=False)\n",
        "    keys_wo__ = []\n",
        "    for keyname in mat.keys():\n",
        "        if not keyname.startswith(\"__\"):\n",
        "            keys_wo__.append(keyname)\n",
        "    target_key = keys_wo__[0]\n",
        "    target_data = np.asarray(mat[target_key], dtype=np.float32)\n",
        "\n",
        "    if target_data.shape[0] == 129:\n",
        "        target_data = target_data[:128, :]\n",
        "\n",
        "    return target_data"
      ],
      "metadata": {
        "id": "mqbe0dmqOgaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tentative Simple Model For Demo"
      ],
      "metadata": {
        "id": "MIQR_h-rc_6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np\n",
        "\n",
        "subj_features, subj_labels, subj_ids = [], [], []\n",
        "\n",
        "for mat_path in mat_files:\n",
        "    subj_id, subj_label = get_subj_id_and_label(mat_path)\n",
        "    raw_data = load_mat(mat_path, verbose=False)\n",
        "    preprocessed_data = preprocess_data(raw_data, SAMPLING_RATE)\n",
        "    wins_array, _ = slide_wins(preprocessed_data, SAMPLING_RATE)\n",
        "    clean_wins = discard_noisy_wins(wins_array)\n",
        "    features_array = welch(wins_array, SAMPLING_RATE)\n",
        "    subj_features.append(features_array.mean(axis=0))\n",
        "    subj_labels.append(subj_label)\n",
        "    subj_ids.append(subj_id)\n",
        "\n",
        "subj_features = np.vstack(subj_features)\n",
        "subj_labels = np.array(subj_labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(subj_features, subj_labels, test_size=0.2, random_state=SEED, stratify=subj_labels)\n",
        "\n",
        "model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=100, random_state=SEED))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Test Accuracy: {model.score(X_test, y_test):.3f}\")"
      ],
      "metadata": {
        "id": "VAv5b9kMdCxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "gO_jis_vdElq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "model_save_path = '/content/eeg_depression_model.pkl'\n",
        "joblib.dump(model, model_save_path)"
      ],
      "metadata": {
        "id": "9EgL9CLbdGlh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}